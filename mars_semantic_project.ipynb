{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/timuriz/ObjectSegmentationMarsWS25/blob/all_in_one_file_ipynb/mars_semantic_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DATASET PREPARATION\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "h1GeONvLjh1W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, cv2 # os - files, oper. sys., cv2 - image processing\n",
        "import numpy as np # arrays\n",
        "import pandas as pd # excel\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "\"\"\"\n",
        "# links on folders with dataset\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "data_folder = '/content/drive/MyDrive/Mashine Learning/Project/AI4Mars_dataset' # link of dataset folder\n",
        "\n",
        "img_train_folder = '/content/drive/Othercomputers/Ноутбук/train_images'\n",
        "mask_train_folder = '/content/drive/Othercomputers/Ноутбук/train_labels'\n",
        "\n",
        "img_valid_folder = '/content/drive/Othercomputers/Ноутбук/valid_images'\n",
        "mask_valid_folder = '/content/drive/Othercomputers/Ноутбук/valid_masks'\n",
        "\n",
        "img_test_folder = '/content/drive/Othercomputers/Ноутбук/test_images'\n",
        "mask_test_folder = '/content/drive/Othercomputers/Ноутбук/test_labels'"
      ],
      "metadata": {
        "id": "4NvlOH-4Hzh8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cee28a8e-d42f-4f97-9ec6-b9d272b6218c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "\n",
        "# get name of classes and their values\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "class_dataframe = pd.read_csv('/content/drive/MyDrive/University study/Mashine Learning/Project/labels.csv') # pandas dataframe\n",
        "\n",
        "class_names = class_dataframe['name'].tolist()\n",
        "\n",
        "class_gray_values = class_dataframe['gray'].values.tolist()\n",
        "\n",
        "\n",
        "print('Class Names: ', class_names)\n",
        "print('Class gray values: ', class_gray_values)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jZ8_Ep-QZxn_",
        "outputId": "5a1a9bfc-9b77-42a4-9f03-144202d88507"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class Names:  ['soil', 'bedrock', 'sand', 'big rock', 'rest']\n",
            "Class gray values:  [0, 1, 2, 3, 255]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# implementation of the Dataset class\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "class CreatImageDataset(Dataset):\n",
        "\n",
        "    def __init__(self, img_folder, mask_folder, transforms):\n",
        "      self.img_folder = img_folder\n",
        "      self.mask_folder = mask_folder\n",
        "      self.transforms = transforms\n",
        "\n",
        "      self.img_paths = os.listdir(self.img_folder)\n",
        "      self.img_paths.sort()\n",
        "\n",
        "      self.mask_paths = os.listdir(self.mask_folder)\n",
        "      self.mask_paths.sort()\n",
        "\n",
        "\n",
        "    # return the amount of files\n",
        "    def __len__(self):\n",
        "        return len(self.img_paths)\n",
        "\n",
        "    # form tensors of the input images and masks\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = os.path.join(self.img_folder, self.img_paths[idx])\n",
        "        img = cv2.imread(img_path, cv2.IMREAD_COLOR)\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) # get Numpy arrays\n",
        "\n",
        "        mask_path = os.path.join(self.mask_folder, self.mask_paths[idx])\n",
        "        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE) # get Numpy arrays\n",
        "\n",
        "        # apply an augmentation\n",
        "        augmented = self.transforms(image=img, mask=mask)\n",
        "        img = augmented['image']\n",
        "        mask = augmented['mask']\n",
        "        mask = mask.long()\n",
        "\n",
        "        return img, mask\n"
      ],
      "metadata": {
        "id": "anf6Fp_OvxyH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "AUGMENTATION"
      ],
      "metadata": {
        "id": "N7CpZO5yUUPA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "# augmentation functions\n",
        "\n",
        "\"\"\"\n",
        "import albumentations as aug\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "\n",
        "\n",
        "def get_train_transforms(image_size=(512, 512)):\n",
        "    h, w = image_size\n",
        "    return aug.Compose([     # albumentations transformation pipeline builder\n",
        "      aug.Resize(height=h, width=w, interpolation=0), # resize the image and the mask, apply nearest-neighbor interpolation\n",
        "      aug.RandomCrop(height=int(h * 0.9), width=int(w * 0.9), p=0.5), # crop images and mask with probability 50%(0.5) to decide to convert or not\n",
        "      aug.HorizontalFlip(p=0.5), # flip with probability 50%\n",
        "      aug.VerticalFlip(p=0.1),\n",
        "      aug.Affine(rotate=(-15, 15), scale=(0.9, 1.1), translate_percent=(0.0625, 0.0625), border_mode=0, p=0.5),  # Rotate → rotate around the center (-15% to +15%)\n",
        "                                                                                                                 # Scale → zoom in or out, scale factor [0.9, 1.1]\n",
        "                                                                                                                 # shift → new pixels on the left are filled with a value 0 (shift on 6.25% of image size)\n",
        "                                                                                                                 # border_mode=0 - fill outside area with 0 pixels\n",
        "      aug.RandomRotate90(p=0.25),\n",
        "      aug.Normalize(mean=(0.485, 0.456, 0.406),\n",
        "            std=(0.229, 0.224, 0.225)),  # image only, uses ImageNet mean/std by default if we have, else 0-1 normalization\n",
        "      ToTensorV2() # converts NumPy arrays into PyTorch tensors\n",
        "    ], additional_targets={}) # no additional masks\n",
        "\n",
        "def get_valid_transforms(image_size=(512, 512)):\n",
        "    h, w = image_size\n",
        "    return aug.Compose([\n",
        "        aug.Resize(height=h, width=w, interpolation=0),\n",
        "        aug.Normalize(mean=(0.485, 0.456, 0.406),\n",
        "            std=(0.229, 0.224, 0.225)),\n",
        "        ToTensorV2()\n",
        "    ])\n"
      ],
      "metadata": {
        "id": "ChgjvdXgDT55"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# visualization\n",
        "\n"
      ],
      "metadata": {
        "id": "BQmyXovUD0Gl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "\n",
        "visualization function to preview images and masks\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def visualize(**images) -> None: # function does not return anything\n",
        "\n",
        "    n = len(images)\n",
        "    plt.figure(figsize=(16, 5))\n",
        "    for i, (name, image) in enumerate(images.items()):\n",
        "        plt.subplot(1, n, i + 1)\n",
        "        plt.xticks([]) # remove tick marks and numbers from the x- and y-axes\n",
        "        plt.yticks([]) # remove tick marks and numbers from the x- and y-axes\n",
        "        plt.title(\" \".join(name.split(\"_\")).title()) # formats and sets the title above the image\n",
        "        plt.imshow(image)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "FQ53Nnn59s9C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "\n",
        "the class for unnormolize image for preliminary visualization\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "class UnNormalize(object):\n",
        "    def __init__(self, mean, std):\n",
        "        self.mean = mean\n",
        "        self.std = std\n",
        "\n",
        "    def __call__(self, tensor):\n",
        "\n",
        "        for t, m, s in zip(tensor, self.mean, self.std):\n",
        "            t.mul_(s).add_(m) # apply the return function (x = x_norm ​× s + m)\n",
        "\n",
        "        return tensor\n",
        "\n",
        "\n",
        "unnorm = UnNormalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))"
      ],
      "metadata": {
        "id": "fJx-X9zU7950"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_transform = get_train_transforms()\n",
        "train_dataset = CreatImageDataset(img_train_folder, mask_train_folder, train_transform)\n",
        "\n",
        "test_transform = get_valid_transforms()\n",
        "test_dataset = CreatImageDataset(img_test_folder, mask_test_folder, test_transform)\n",
        "\n",
        "valid_transform = get_valid_transforms()\n",
        "valid_dataset = CreatImageDataset(img_valid_folder, mask_valid_folder, valid_transform)"
      ],
      "metadata": {
        "id": "SjdubkXpiQ7-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "\n",
        "call visualization\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "idx = np.random.randint(len(train_dataset))\n",
        "image, mask = train_dataset[idx]\n",
        "visualize(image=unnorm(image).permute(1, 2, 0), mask=mask)"
      ],
      "metadata": {
        "id": "Slg7Np7WCiWf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "DATALOADER"
      ],
      "metadata": {
        "id": "cPfB1ZzI7jZH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "n_workers = os.cpu_count()\n",
        "print(\"num_workers = \", n_workers) # show how many CPU cores are available\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True, # randomly shuffles the order of samples at the beginning of each epoch.\n",
        "    num_workers=n_workers,\n",
        "    drop_last=True, # last smaller batch is discarded\n",
        ")\n",
        "\n",
        "test_dataloader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    num_workers=n_workers,\n",
        "    drop_last=False,\n",
        ")\n",
        "\n",
        "valid_dataloader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    num_workers=n_workers,\n",
        "    drop_last=False,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JFHTP6q6FwMr",
        "outputId": "b958de24-d180-4a1e-b102-20375d99d770"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num_workers =  2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MODEL\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "oRp-ThHxVt5L"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rJ--Z3rRoQ0h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install segmentation-models-pytorch\n",
        "\n",
        "import torch\n",
        "import segmentation_models_pytorch as smp\n",
        "\n",
        "model = smp.DeepLabV3Plus(\n",
        "    encoder_name=\"resnet101\",        # ResNet-101 backbone\n",
        "    encoder_weights=\"imagenet\",      # ImageNet pretrained\n",
        "    in_channels=3,                   # how many channels the input image has\n",
        "    classes=5                        # amount of classes\n",
        ")\n",
        "\n",
        "images, masks = next(iter(train_dataloader)) # iterator and next batch\n",
        "outputs = model(images) # deeplab return dictionary \"out\" and \"aux\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 743,
          "referenced_widgets": [
            "909fc8f0914b4797867f819db2cce583",
            "688d319258cc4ec28c5b13ce3b269b40",
            "677bf1258fd54420a036d986f4a7d8c6",
            "6edae6138ca64774b413bd882d34974a",
            "7e9c549708324f8d962d0679dcffbb28",
            "01def7486ed0484992e83b13c679f4b3",
            "f5067d3016d7467e9720c5b2fb45ce7d",
            "9c5102f93dfb4fca8a606d799b52c27b",
            "6d85d4a2d48e4253ab82ae0db42d36fe",
            "0ad0d5a9135747afba43fc373c23e768",
            "0c30fc543b094efc90caf0d2365cd684",
            "e23665cac927454389bbb3613f7f623c",
            "6b0300ce88154f4ab7615541004620b0",
            "36080cd86e73464eab74fc1cf0fb6780",
            "a875e3bf15f6484fab230013d528f6e2",
            "b7b83b69e6574719a5ef5c44fbb56ae7",
            "d622951a8aab45fe97198edc44cb082b",
            "dbfd8cd793354e93b73daf44b451443a",
            "f04230228a0e4b61b53d2f0141534abf",
            "553b923d60c84ccdb13452f0a72c0d9b",
            "4a6553b193dd4f62ac42a9ab43ec6b60",
            "64c680845ee244b0aea4a83fb7ea45fb"
          ]
        },
        "id": "c6u-voCUPwJf",
        "outputId": "e175e79e-63b4-4388-b31d-9e03e325e911"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting segmentation-models-pytorch\n",
            "  Downloading segmentation_models_pytorch-0.5.0-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: huggingface-hub>=0.24 in /usr/local/lib/python3.12/dist-packages (from segmentation-models-pytorch) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.19.3 in /usr/local/lib/python3.12/dist-packages (from segmentation-models-pytorch) (2.0.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from segmentation-models-pytorch) (11.3.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.12/dist-packages (from segmentation-models-pytorch) (0.7.0)\n",
            "Requirement already satisfied: timm>=0.9 in /usr/local/lib/python3.12/dist-packages (from segmentation-models-pytorch) (1.0.22)\n",
            "Requirement already satisfied: torch>=1.8 in /usr/local/lib/python3.12/dist-packages (from segmentation-models-pytorch) (2.9.0+cpu)\n",
            "Requirement already satisfied: torchvision>=0.9 in /usr/local/lib/python3.12/dist-packages (from segmentation-models-pytorch) (0.24.0+cpu)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from segmentation-models-pytorch) (4.67.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24->segmentation-models-pytorch) (3.20.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24->segmentation-models-pytorch) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24->segmentation-models-pytorch) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24->segmentation-models-pytorch) (6.0.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24->segmentation-models-pytorch) (2.32.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24->segmentation-models-pytorch) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24->segmentation-models-pytorch) (1.2.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation-models-pytorch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation-models-pytorch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation-models-pytorch) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation-models-pytorch) (3.1.6)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.8->segmentation-models-pytorch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.8->segmentation-models-pytorch) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.24->segmentation-models-pytorch) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.24->segmentation-models-pytorch) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.24->segmentation-models-pytorch) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.24->segmentation-models-pytorch) (2025.11.12)\n",
            "Downloading segmentation_models_pytorch-0.5.0-py3-none-any.whl (154 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.8/154.8 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: segmentation-models-pytorch\n",
            "Successfully installed segmentation-models-pytorch-0.5.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/156 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "909fc8f0914b4797867f819db2cce583"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/179M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e23665cac927454389bbb3613f7f623c"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TRAINING\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "3GnymIQ8Vx3k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EVALUATION\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "qJzi2gCNV4Pn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DEPLOYMENT\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "ARYRIsfnV-Xi"
      }
    }
  ]
}