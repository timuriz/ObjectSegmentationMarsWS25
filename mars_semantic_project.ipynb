{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/timuriz/ObjectSegmentationMarsWS25/blob/all_in_one_file_ipynb/mars_semantic_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DATASET PREPARATION\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "h1GeONvLjh1W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, cv2 # os - files, oper. sys., cv2 - image processing\n",
        "import numpy as np # arrays\n",
        "import pandas as pd # excel\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "\"\"\"\n",
        "# links on folders with dataset\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "data_folder = '/content/drive/MyDrive/Mashine Learning/Project/AI4Mars_dataset' # link of dataset folder\n",
        "\n",
        "img_train_folder = '/content/drive/Othercomputers/Ноутбук/train_images'\n",
        "mask_train_folder = '/content/drive/Othercomputers/Ноутбук/train_labels'\n",
        "\n",
        "img_valid_folder = '/content/drive/Othercomputers/Ноутбук/valid_images'\n",
        "mask_valid_folder = '/content/drive/Othercomputers/Ноутбук/valid_masks'\n",
        "\n",
        "img_test_folder = '/content/drive/Othercomputers/Ноутбук/test_images'\n",
        "mask_test_folder = '/content/drive/Othercomputers/Ноутбук/test_labels'"
      ],
      "metadata": {
        "id": "4NvlOH-4Hzh8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cee28a8e-d42f-4f97-9ec6-b9d272b6218c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "\n",
        "# get name of classes and their values\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "class_dataframe = pd.read_csv('/content/drive/MyDrive/University study/Mashine Learning/Project/labels.csv') # pandas dataframe\n",
        "\n",
        "class_names = class_dataframe['name'].tolist()\n",
        "\n",
        "class_gray_values = class_dataframe['gray'].values.tolist()\n",
        "\n",
        "\n",
        "print('Class Names: ', class_names)\n",
        "print('Class gray values: ', class_gray_values)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jZ8_Ep-QZxn_",
        "outputId": "5a1a9bfc-9b77-42a4-9f03-144202d88507"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class Names:  ['soil', 'bedrock', 'sand', 'big rock', 'rest']\n",
            "Class gray values:  [0, 1, 2, 3, 255]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# implementation of the Dataset class\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "class CreatImageDataset(Dataset):\n",
        "\n",
        "    def __init__(self, img_folder, mask_folder, transforms):\n",
        "      self.img_folder = img_folder\n",
        "      self.mask_folder = mask_folder\n",
        "      self.transforms = transforms\n",
        "\n",
        "      self.img_paths = os.listdir(self.img_folder)\n",
        "      self.img_paths.sort()\n",
        "\n",
        "      self.mask_paths = os.listdir(self.mask_folder)\n",
        "      self.mask_paths.sort()\n",
        "\n",
        "\n",
        "    # return the amount of files\n",
        "    def __len__(self):\n",
        "        return len(self.img_paths)\n",
        "\n",
        "    # form tensors of the input images and masks\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = os.path.join(self.img_folder, self.img_paths[idx])\n",
        "        img = cv2.imread(img_path, cv2.IMREAD_COLOR)\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) # get Numpy arrays\n",
        "\n",
        "        mask_path = os.path.join(self.mask_folder, self.mask_paths[idx])\n",
        "        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE) # get Numpy arrays\n",
        "\n",
        "        # apply an augmentation\n",
        "        augmented = self.transforms(image=img, mask=mask)\n",
        "        img = augmented['image']\n",
        "        mask = augmented['mask']\n",
        "        mask = mask.long()\n",
        "\n",
        "        return img, mask\n"
      ],
      "metadata": {
        "id": "anf6Fp_OvxyH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "AUGMENTATION"
      ],
      "metadata": {
        "id": "N7CpZO5yUUPA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "# augmentation functions\n",
        "\n",
        "\"\"\"\n",
        "import albumentations as aug\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "\n",
        "\n",
        "def get_train_transforms(image_size=(512, 512)):\n",
        "    h, w = image_size\n",
        "    return aug.Compose([     # albumentations transformation pipeline builder\n",
        "      aug.Resize(height=h, width=w, interpolation=0), # resize the image and the mask, apply nearest-neighbor interpolation\n",
        "      aug.RandomCrop(height=int(h * 0.9), width=int(w * 0.9), p=0.5), # crop images and mask with probability 50%(0.5) to decide to convert or not\n",
        "      aug.HorizontalFlip(p=0.5), # flip with probability 50%\n",
        "      aug.VerticalFlip(p=0.1),\n",
        "      aug.Affine(rotate=(-15, 15), scale=(0.9, 1.1), translate_percent=(0.0625, 0.0625), border_mode=0, p=0.5),  # Rotate → rotate around the center (-15% to +15%)\n",
        "                                                                                                                 # Scale → zoom in or out, scale factor [0.9, 1.1]\n",
        "                                                                                                                 # shift → new pixels on the left are filled with a value 0 (shift on 6.25% of image size)\n",
        "                                                                                                                 # border_mode=0 - fill outside area with 0 pixels\n",
        "      aug.RandomRotate90(p=0.25),\n",
        "      aug.Normalize(mean=(0.485, 0.456, 0.406),\n",
        "            std=(0.229, 0.224, 0.225)),  # image only, uses ImageNet mean/std by default if we have, else 0-1 normalization\n",
        "      ToTensorV2() # converts NumPy arrays into PyTorch tensors\n",
        "    ], additional_targets={}) # no additional masks\n",
        "\n",
        "def get_valid_transforms(image_size=(512, 512)):\n",
        "    h, w = image_size\n",
        "    return aug.Compose([\n",
        "        aug.Resize(height=h, width=w, interpolation=0),\n",
        "        aug.Normalize(mean=(0.485, 0.456, 0.406),\n",
        "            std=(0.229, 0.224, 0.225)),\n",
        "        ToTensorV2()\n",
        "    ])\n"
      ],
      "metadata": {
        "id": "ChgjvdXgDT55"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# visualization\n",
        "\n"
      ],
      "metadata": {
        "id": "BQmyXovUD0Gl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "\n",
        "visualization function to preview images and masks\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def visualize(**images) -> None: # function does not return anything\n",
        "\n",
        "    n = len(images)\n",
        "    plt.figure(figsize=(16, 5))\n",
        "    for i, (name, image) in enumerate(images.items()):\n",
        "        plt.subplot(1, n, i + 1)\n",
        "        plt.xticks([]) # remove tick marks and numbers from the x- and y-axes\n",
        "        plt.yticks([]) # remove tick marks and numbers from the x- and y-axes\n",
        "        plt.title(\" \".join(name.split(\"_\")).title()) # formats and sets the title above the image\n",
        "        plt.imshow(image)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "FQ53Nnn59s9C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "\n",
        "the class for unnormolize image for preliminary visualization\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "class UnNormalize(object):\n",
        "    def __init__(self, mean, std):\n",
        "        self.mean = mean\n",
        "        self.std = std\n",
        "\n",
        "    def __call__(self, tensor):\n",
        "\n",
        "        for t, m, s in zip(tensor, self.mean, self.std):\n",
        "            t.mul_(s).add_(m) # apply the return function (x = x_norm ​× s + m)\n",
        "\n",
        "        return tensor\n",
        "\n",
        "\n",
        "unnorm = UnNormalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))"
      ],
      "metadata": {
        "id": "fJx-X9zU7950"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_transform = get_train_transforms()\n",
        "train_dataset = CreatImageDataset(img_train_folder, mask_train_folder, train_transform)\n",
        "\n",
        "test_transform = get_valid_transforms()\n",
        "test_dataset = CreatImageDataset(img_test_folder, mask_test_folder, test_transform)\n",
        "\n",
        "valid_transform = get_valid_transforms()\n",
        "valid_dataset = CreatImageDataset(img_valid_folder, mask_valid_folder, valid_transform)"
      ],
      "metadata": {
        "id": "SjdubkXpiQ7-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "\n",
        "call visualization\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "idx = np.random.randint(len(train_dataset))\n",
        "image, mask = train_dataset[idx]\n",
        "visualize(image=unnorm(image).permute(1, 2, 0), mask=mask)"
      ],
      "metadata": {
        "id": "Slg7Np7WCiWf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "DATALOADER"
      ],
      "metadata": {
        "id": "cPfB1ZzI7jZH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "n_workers = os.cpu_count()\n",
        "print(\"num_workers = \", n_workers) # show how many CPU cores are available\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True, # randomly shuffles the order of samples at the beginning of each epoch.\n",
        "    num_workers=n_workers,\n",
        "    drop_last=True, # last smaller batch is discarded\n",
        ")\n",
        "\n",
        "test_dataloader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    num_workers=n_workers,\n",
        "    drop_last=False,\n",
        ")\n",
        "\n",
        "valid_dataloader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    num_workers=n_workers,\n",
        "    drop_last=False,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JFHTP6q6FwMr",
        "outputId": "b958de24-d180-4a1e-b102-20375d99d770"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num_workers =  2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MODEL\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "oRp-ThHxVt5L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TRAINING\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "3GnymIQ8Vx3k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EVALUATION\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "qJzi2gCNV4Pn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DEPLOYMENT\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "ARYRIsfnV-Xi"
      }
    }
  ]
}