{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/timuriz/ObjectSegmentationMarsWS25/blob/all_in_one_file_ipynb/mars_semantic_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DATASET PREPARATION\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "h1GeONvLjh1W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, cv2 # os - files, oper. sys., cv2 - image processing\n",
        "import numpy as np # arrays\n",
        "import pandas as pd # excel\n",
        "\n",
        "\"\"\"\n",
        "# links on folders with dataset\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "data_folder = 'https://drive.google.com/drive/folders/1BIzPHVNSIMxCZi_lD4kqXwi0QAVrnYNy?usp=sharing' # link of dataset folder\n",
        "\n",
        "img_train_folder = os.path.join(data_folder, 'train')\n",
        "mask_train_folder = os.path.join(data_folder, 'train_labels')\n",
        "\n",
        "img_valid_folder = os.path.join(data_folder, 'valid')\n",
        "mask_valid_folder = os.path.join(data_folder, 'valid_labels')\n",
        "\n",
        "img_test_folder = os.path.join(data_folder, 'test')\n",
        "mask_test_folder = os.path.join(data_folder, 'test_labels')"
      ],
      "metadata": {
        "id": "4NvlOH-4Hzh8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "\n",
        "# get name of classes and their values\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "class_dataframe = pd.read_csv('https://docs.google.com/spreadsheets/d/1yMCRB7ckbci1lfNnwa9XPnflYxrNs8Y2/export?format=csv') # pandas dataframe\n",
        "\n",
        "class_names = class_dataframe['name'].tolist()\n",
        "\n",
        "class_gray_values = class_dataframe['gray'].values.tolist()\n",
        "\n",
        "\n",
        "print('Class Names: ', class_names)\n",
        "print('Class gray values: ', class_gray_values)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jZ8_Ep-QZxn_",
        "outputId": "5a1a9bfc-9b77-42a4-9f03-144202d88507"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class Names:  ['soil', 'bedrock', 'sand', 'big rock', 'rest']\n",
            "Class gray values:  [0, 1, 2, 3, 255]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# implementation of the Dataset class\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "class CreatImageDataset(Dataset):\n",
        "\n",
        "  def __init__(self, img_folder, mask_folder, transforms):\n",
        "    self.img_folder = img_folder\n",
        "    self.mask_folder = mask_folder\n",
        "    self.transforms = transforms\n",
        "\n",
        "    self.img_paths = os.listdir(self.img_folder)\n",
        "    self.img_paths.sort()\n",
        "\n",
        "    self.mask_paths = os.listdir(self.mask_folder)\n",
        "    self.mask_paths.sort()\n",
        "\n",
        "\n",
        "    # return the amount of files\n",
        "    def __len__(self):\n",
        "        return len(self.imgs_paths)\n",
        "\n",
        "    # form tensors of the input images and masks\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = os.path.join(self.img_folder, self.img_paths[idx])\n",
        "        img = cv2.imread(img_path, cv2.IMREAD_COLOR)\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) # get Numpy arrays\n",
        "\n",
        "        mask_path = os.path.join(self.mask_folder, self.mask_paths[idx])\n",
        "        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE) # get Numpy arrays\n",
        "\n",
        "        # apply an augmentation\n",
        "        augmented = self.transforms(image=img, mask=mask)\n",
        "        img = augmented['image']\n",
        "        mask = augmented['mask']\n",
        "\n",
        "    mask = mask.long()\n",
        "\n",
        "    return img, mask\n"
      ],
      "metadata": {
        "id": "anf6Fp_OvxyH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "AUGMENTATION"
      ],
      "metadata": {
        "id": "N7CpZO5yUUPA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "# augmentation functions\n",
        "\n",
        "\"\"\"\n",
        "import albumentations as aug\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "\n",
        "\n",
        "def get_train_transforms(image_size=(512, 512)):\n",
        "    h, w = image_size\n",
        "    return aug.Compose([     # albumentations transformation pipeline builder\n",
        "      aug.Resize(height=h, width=w, interpolation=0), # resize the image and the mask, apply nearest-neighbor interpolation\n",
        "      aug.RandomCrop(height=int(h * 0.9), width=int(w * 0.9), p=0.5), # crop images and mask with probability 50%(0.5) to decide to convert or not\n",
        "      aug.HorizontalFlip(p=0.5), # flip with probability 50%\n",
        "      aug.VerticalFlip(p=0.1),\n",
        "      A.ShiftScaleRotate(rotate_limit=15, scale_limit=0.1, shift_limit=0.0625, p=0.5, border_mode=0), # Rotate → rotate around the center (-15% to +15%)\n",
        "                                                                                                      # Scale → zoom in or out, scale factor [0.9, 1.1]\n",
        "                                                                                                      # shift → new pixels on the left are filled with a value 0 (shift on 6.25% of image size)\n",
        "                                                                                                      # border_mode=0 - fill outside area with 0 pixels\n",
        "      aug.RandomRotate90(p=0.25),\n",
        "      aug.Normalize(mean=(0.485, 0.456, 0.406),\n",
        "            std=(0.229, 0.224, 0.225)),  # image only, uses ImageNet mean/std by default if we have, else 0-1 normalization\n",
        "      ToTensorV2() # converts NumPy arrays into PyTorch tensors\n",
        "    ], additional_targets={}) # no additional masks\n",
        "\n",
        "def get_valid_transforms(image_size=(512, 512)):\n",
        "    h, w = image_size\n",
        "    return aug.Compose([\n",
        "        aug.Resize(height=h, width=w, interpolation=0),\n",
        "        aug.Normalize(mean=(0.485, 0.456, 0.406),\n",
        "            std=(0.229, 0.224, 0.225)),\n",
        "        ToTensorV2()\n",
        "    ])\n"
      ],
      "metadata": {
        "id": "ChgjvdXgDT55"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MODEL\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "oRp-ThHxVt5L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TRAINING\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "3GnymIQ8Vx3k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EVALUATION\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "qJzi2gCNV4Pn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DEPLOYMENT\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "ARYRIsfnV-Xi"
      }
    }
  ]
}